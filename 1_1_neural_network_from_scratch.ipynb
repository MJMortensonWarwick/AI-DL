{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyv00LbhA01V7uuouDFJ0D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/AI-DL/blob/main/1_1_neural_network_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network from Scratch (with TensorFlow)\n",
        "This first Notebook will take us through building our first neural network, building the network from the ground up (as much as possible). If you haven't already, be sure to switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqroCOII9aK2",
        "outputId": "5fddd363-ae69-4258-ebb0-1627fb58758b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with an old favourite:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the code from the last module\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "ebeaf485-7261-44cb-d579-23b6848c952c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n  :Number of Instances: 442\\n\\n  :Number of Attributes: First 10 columns are numeric predictive values\\n\\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n  :Attribute Information:\\n      - age     age in years\\n      - sex\\n      - bmi     body mass index\\n      - bp      average blood pressure\\n      - s1      tc, total serum cholesterol\\n      - s2      ldl, low-density lipoproteins\\n      - s3      hdl, high-density lipoproteins\\n      - s4      tch, total cholesterol / HDL\\n      - s5      ltg, possibly log of serum triglycerides level\\n      - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)',\n",
              " 'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990842, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06832974, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286377, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04687948,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452837, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00421986,  0.00306441]]),\n",
              " 'data_filename': 'diabetes_data.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'frame': None,\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'target_filename': 'diabetes_target.csv.gz'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and standardised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into TensorFlow:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = tf.data.Dataset.from_tensor_slices((data.data, data.target))"
      ],
      "metadata": {
        "id": "azRdYAfD_-Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before. However, as we will care about batch size lets pick an easy number to work with:"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_size = len(data.data)\n",
        "print(data_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHHWvZzlBKd6",
        "outputId": "911781cf-fa90-4e03-e922-2a720a185165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given 442 records we can take 400 as training (roughly 90% ... it is a small dataset) and keep 42 for test. In TF we use _take_ to subset the first $n$ values and _skip_ to ignore these and subset the rest:"
      ],
      "metadata": {
        "id": "EIxmeeY3Ogv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf_dataset.take(400)\n",
        "test_dataset = tf_dataset.skip(400)"
      ],
      "metadata": {
        "id": "2j_zJwhWOhNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = train_dataset.batch(50)\n",
        "features, labels = next(iter(train_batch))\n",
        "\n",
        "print(features)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgI1OiXhJi0V",
        "outputId": "862e8a33-8256-48f2-b769-6ac0b97d2de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 3.80759064e-02  5.06801187e-02  6.16962065e-02  2.18723550e-02\n",
            "  -4.42234984e-02 -3.48207628e-02 -4.34008457e-02 -2.59226200e-03\n",
            "   1.99084209e-02 -1.76461252e-02]\n",
            " [-1.88201653e-03 -4.46416365e-02 -5.14740612e-02 -2.63278347e-02\n",
            "  -8.44872411e-03 -1.91633397e-02  7.44115641e-02 -3.94933829e-02\n",
            "  -6.83297436e-02 -9.22040496e-02]\n",
            " [ 8.52989063e-02  5.06801187e-02  4.44512133e-02 -5.67061055e-03\n",
            "  -4.55994513e-02 -3.41944659e-02 -3.23559322e-02 -2.59226200e-03\n",
            "   2.86377052e-03 -2.59303390e-02]\n",
            " [-8.90629394e-02 -4.46416365e-02 -1.15950145e-02 -3.66564468e-02\n",
            "   1.21905688e-02  2.49905934e-02 -3.60375700e-02  3.43088589e-02\n",
            "   2.26920226e-02 -9.36191133e-03]\n",
            " [ 5.38306037e-03 -4.46416365e-02 -3.63846922e-02  2.18723550e-02\n",
            "   3.93485161e-03  1.55961395e-02  8.14208361e-03 -2.59226200e-03\n",
            "  -3.19914449e-02 -4.66408736e-02]\n",
            " [-9.26954778e-02 -4.46416365e-02 -4.06959405e-02 -1.94420933e-02\n",
            "  -6.89906499e-02 -7.92878444e-02  4.12768238e-02 -7.63945038e-02\n",
            "  -4.11803852e-02 -9.63461565e-02]\n",
            " [-4.54724779e-02  5.06801187e-02 -4.71628129e-02 -1.59992226e-02\n",
            "  -4.00956398e-02 -2.48000121e-02  7.78807997e-04 -3.94933829e-02\n",
            "  -6.29129499e-02 -3.83566597e-02]\n",
            " [ 6.35036756e-02  5.06801187e-02 -1.89470584e-03  6.66296740e-02\n",
            "   9.06198817e-02  1.08914381e-01  2.28686348e-02  1.77033545e-02\n",
            "  -3.58167281e-02  3.06440941e-03]\n",
            " [ 4.17084449e-02  5.06801187e-02  6.16962065e-02 -4.00993175e-02\n",
            "  -1.39525355e-02  6.20168566e-03 -2.86742944e-02 -2.59226200e-03\n",
            "  -1.49564750e-02  1.13486232e-02]\n",
            " [-7.09002471e-02 -4.46416365e-02  3.90621530e-02 -3.32135761e-02\n",
            "  -1.25765827e-02 -3.45076144e-02 -2.49926566e-02 -2.59226200e-03\n",
            "   6.77363261e-02 -1.35040182e-02]\n",
            " [-9.63280163e-02 -4.46416365e-02 -8.38084235e-02  8.10087222e-03\n",
            "  -1.03389471e-01 -9.05611890e-02 -1.39477432e-02 -7.63945038e-02\n",
            "  -6.29129499e-02 -3.42145528e-02]\n",
            " [ 2.71782911e-02  5.06801187e-02  1.75059115e-02 -3.32135761e-02\n",
            "  -7.07277125e-03  4.59715403e-02 -6.54906725e-02  7.12099798e-02\n",
            "  -9.64332229e-02 -5.90671943e-02]\n",
            " [ 1.62806757e-02 -4.46416365e-02 -2.88400077e-02 -9.11348125e-03\n",
            "  -4.32086554e-03 -9.76888589e-03  4.49584616e-02 -3.94933829e-02\n",
            "  -3.07512099e-02 -4.24987666e-02]\n",
            " [ 5.38306037e-03  5.06801187e-02 -1.89470584e-03  8.10087222e-03\n",
            "  -4.32086554e-03 -1.57187067e-02 -2.90282981e-03 -2.59226200e-03\n",
            "   3.83932482e-02 -1.35040182e-02]\n",
            " [ 4.53409833e-02 -4.46416365e-02 -2.56065715e-02 -1.25563519e-02\n",
            "   1.76943802e-02 -6.12835791e-05  8.17748397e-02 -3.94933829e-02\n",
            "  -3.19914449e-02 -7.56356220e-02]\n",
            " [-5.27375548e-02  5.06801187e-02 -1.80618869e-02  8.04011568e-02\n",
            "   8.92439288e-02  1.07661787e-01 -3.97192078e-02  1.08111101e-01\n",
            "   3.60557901e-02 -4.24987666e-02]\n",
            " [-5.51455498e-03 -4.46416365e-02  4.22955892e-02  4.94153205e-02\n",
            "   2.45741445e-02 -2.38605667e-02  7.44115641e-02 -3.94933829e-02\n",
            "   5.22799998e-02  2.79170509e-02]\n",
            " [ 7.07687525e-02  5.06801187e-02  1.21168511e-02  5.63010619e-02\n",
            "   3.42058145e-02  4.94161734e-02 -3.97192078e-02  3.43088589e-02\n",
            "   2.73677075e-02 -1.07769750e-03]\n",
            " [-3.82074010e-02 -4.46416365e-02 -1.05172024e-02 -3.66564468e-02\n",
            "  -3.73437341e-02 -1.94764882e-02 -2.86742944e-02 -2.59226200e-03\n",
            "  -1.81182673e-02 -1.76461252e-02]\n",
            " [-2.73097857e-02 -4.46416365e-02 -1.80618869e-02 -4.00993175e-02\n",
            "  -2.94491268e-03 -1.13346282e-02  3.75951860e-02 -3.94933829e-02\n",
            "  -8.94401896e-03 -5.49250874e-02]\n",
            " [-4.91050164e-02 -4.46416365e-02 -5.68631216e-02 -4.35421882e-02\n",
            "  -4.55994513e-02 -4.32757713e-02  7.78807997e-04 -3.94933829e-02\n",
            "  -1.19006848e-02  1.54907302e-02]\n",
            " [-8.54304009e-02  5.06801187e-02 -2.23731352e-02  1.21513083e-03\n",
            "  -3.73437341e-02 -2.63657544e-02  1.55053592e-02 -3.94933829e-02\n",
            "  -7.21284546e-02 -1.76461252e-02]\n",
            " [-8.54304009e-02 -4.46416365e-02 -4.05032999e-03 -9.11348125e-03\n",
            "  -2.94491268e-03  7.76742797e-03  2.28686348e-02 -3.94933829e-02\n",
            "  -6.11765951e-02 -1.35040182e-02]\n",
            " [ 4.53409833e-02  5.06801187e-02  6.06183944e-02  3.10533436e-02\n",
            "   2.87020031e-02 -4.73467013e-02 -5.44457591e-02  7.12099798e-02\n",
            "   1.33598980e-01  1.35611831e-01]\n",
            " [-6.36351702e-02 -4.46416365e-02  3.58287167e-02 -2.28849640e-02\n",
            "  -3.04639698e-02 -1.88501913e-02 -6.58446761e-03 -2.59226200e-03\n",
            "  -2.59524244e-02 -5.49250874e-02]\n",
            " [-6.72677086e-02  5.06801187e-02 -1.26728266e-02 -4.00993175e-02\n",
            "  -1.53284884e-02  4.63594335e-03 -5.81273969e-02  3.43088589e-02\n",
            "   1.91990331e-02 -3.42145528e-02]\n",
            " [-1.07225632e-01 -4.46416365e-02 -7.73415510e-02 -2.63278347e-02\n",
            "  -8.96299427e-02 -9.61978613e-02  2.65502726e-02 -7.63945038e-02\n",
            "  -4.25721049e-02 -5.21980442e-03]\n",
            " [-2.36772472e-02 -4.46416365e-02  5.95405824e-02 -4.00993175e-02\n",
            "  -4.28475456e-02 -4.35889198e-02  1.18237214e-02 -3.94933829e-02\n",
            "  -1.59982678e-02  4.03433716e-02]\n",
            " [ 5.26060602e-02 -4.46416365e-02 -2.12953232e-02 -7.45280244e-02\n",
            "  -4.00956398e-02 -3.76390990e-02 -6.58446761e-03 -3.94933829e-02\n",
            "  -6.09254186e-04 -5.49250874e-02]\n",
            " [ 6.71362140e-02  5.06801187e-02 -6.20595414e-03  6.31868033e-02\n",
            "  -4.28475456e-02 -9.58847129e-02  5.23217373e-02 -7.63945038e-02\n",
            "   5.94238004e-02  5.27696924e-02]\n",
            " [-6.00026317e-02 -4.46416365e-02  4.44512133e-02 -1.94420933e-02\n",
            "  -9.82467697e-03 -7.57684666e-03  2.28686348e-02 -3.94933829e-02\n",
            "  -2.71286456e-02 -9.36191133e-03]\n",
            " [-2.36772472e-02 -4.46416365e-02 -6.54856182e-02 -8.14137658e-02\n",
            "  -3.87196870e-02 -5.36096705e-02  5.96850129e-02 -7.63945038e-02\n",
            "  -3.71283460e-02 -4.24987666e-02]\n",
            " [ 3.44433680e-02  5.06801187e-02  1.25287119e-01  2.87580964e-02\n",
            "  -5.38551684e-02 -1.29003705e-02 -1.02307051e-01  1.08111101e-01\n",
            "   2.71485728e-04  2.79170509e-02]\n",
            " [ 3.08108295e-02 -4.46416365e-02 -5.03962492e-02 -2.22773986e-03\n",
            "  -4.42234984e-02 -8.99348921e-02  1.18591218e-01 -7.63945038e-02\n",
            "  -1.81182673e-02  3.06440941e-03]\n",
            " [ 1.62806757e-02 -4.46416365e-02 -6.33299941e-02 -5.73136710e-02\n",
            "  -5.79830270e-02 -4.89124436e-02  8.14208361e-03 -3.94933829e-02\n",
            "  -5.94726974e-02 -6.73514081e-02]\n",
            " [ 4.89735218e-02  5.06801187e-02 -3.09956318e-02 -4.92803060e-02\n",
            "   4.93412959e-02 -4.13221358e-03  1.33317769e-01 -5.35158088e-02\n",
            "   2.13108466e-02  1.96328371e-02]\n",
            " [ 1.26481373e-02 -4.46416365e-02  2.28949719e-02  5.28581912e-02\n",
            "   8.06271019e-03 -2.85577936e-02  3.75951860e-02 -3.94933829e-02\n",
            "   5.47240033e-02 -2.59303390e-02]\n",
            " [-9.14709343e-03 -4.46416365e-02  1.10390390e-02 -5.73136710e-02\n",
            "  -2.49601584e-02 -4.29626228e-02  3.02319104e-02 -3.94933829e-02\n",
            "   1.70371324e-02 -5.21980442e-03]\n",
            " [-1.88201653e-03  5.06801187e-02  7.13965152e-02  9.76155103e-02\n",
            "   8.78679760e-02  7.54074957e-02 -2.13110188e-02  7.12099798e-02\n",
            "   7.14240328e-02  2.37749440e-02]\n",
            " [-1.88201653e-03  5.06801187e-02  1.42724753e-02 -7.45280244e-02\n",
            "   2.55889875e-03  6.20168566e-03 -1.39477432e-02 -2.59226200e-03\n",
            "   1.91990331e-02  3.06440941e-03]\n",
            " [ 5.38306037e-03  5.06801187e-02 -8.36157828e-03  2.18723550e-02\n",
            "   5.48451074e-02  7.32154565e-02 -2.49926566e-02  3.43088589e-02\n",
            "   1.25531528e-02  9.41907615e-02]\n",
            " [-9.99605547e-02 -4.46416365e-02 -6.76412423e-02 -1.08956731e-01\n",
            "  -7.44944613e-02 -7.27117267e-02  1.55053592e-02 -3.94933829e-02\n",
            "  -4.98684677e-02 -9.36191133e-03]\n",
            " [-6.00026317e-02  5.06801187e-02 -1.05172024e-02 -1.48515991e-02\n",
            "  -4.97273099e-02 -2.35474182e-02 -5.81273969e-02  1.58582984e-02\n",
            "  -9.91895736e-03 -3.42145528e-02]\n",
            " [ 1.99132142e-02 -4.46416365e-02 -2.34509473e-02 -7.10851537e-02\n",
            "   2.04462859e-02 -1.00820344e-02  1.18591218e-01 -7.63945038e-02\n",
            "  -4.25721049e-02  7.34802270e-02]\n",
            " [ 4.53409833e-02  5.06801187e-02  6.81630790e-02  8.10087222e-03\n",
            "  -1.67044413e-02  4.63594335e-03 -7.65355859e-02  7.12099798e-02\n",
            "   3.24332258e-02 -1.76461252e-02]\n",
            " [ 2.71782911e-02  5.06801187e-02 -3.53068801e-02  3.22009671e-02\n",
            "  -1.12006298e-02  1.50445873e-03 -1.02661054e-02 -2.59226200e-03\n",
            "  -1.49564750e-02 -5.07829805e-02]\n",
            " [-5.63700933e-02 -4.46416365e-02 -1.15950145e-02 -3.32135761e-02\n",
            "  -4.69754041e-02 -4.76598498e-02  4.46044580e-03 -3.94933829e-02\n",
            "  -7.97939755e-03 -8.80619427e-02]\n",
            " [-7.81653240e-02 -4.46416365e-02 -7.30303027e-02 -5.73136710e-02\n",
            "  -8.41261313e-02 -7.42774690e-02 -2.49926566e-02 -3.94933829e-02\n",
            "  -1.81182673e-02 -8.39198358e-02]\n",
            " [ 6.71362140e-02  5.06801187e-02 -4.17737526e-02  1.15437429e-02\n",
            "   2.55889875e-03  5.88853719e-03  4.12768238e-02 -3.94933829e-02\n",
            "  -5.94726974e-02 -2.17882321e-02]\n",
            " [-4.18399395e-02  5.06801187e-02  1.42724753e-02 -5.67061055e-03\n",
            "  -1.25765827e-02  6.20168566e-03 -7.28539481e-02  7.12099798e-02\n",
            "   3.54619387e-02 -1.35040182e-02]], shape=(50, 10), dtype=float64)\n",
            "tf.Tensor(\n",
            "[151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
            " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
            " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
            "  61.  92. 259.  53. 190. 142.  75. 142.], shape=(50,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 20 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(20, activation=tf.nn.relu, input_shape=(10, )),  # 10 features\n",
        "  tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our model built we can define a loss function ... MSE in this case. We can also make a prediction on a single batch:"
      ],
      "metadata": {
        "id": "c7aKO8_iEwCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "def loss(model, x, y):\n",
        "  y_ = model(x)\n",
        "  return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "l = loss(model, features, labels)\n",
        "print(\"Loss test: {}\".format(l))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EudMpQQgEwZz",
        "outputId": "c22c9239-2552-4c6a-bdbd-aea5a511e760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss test: 25657.927734375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a great loss/MSE rate ... but the model hasn't been trained yet 😸\n",
        "\n",
        "Let's get on with the training:"
      ],
      "metadata": {
        "id": "QW_AEc3NKvQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "loss_value, grads = grad(model, features, labels)\n",
        "\n",
        "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "                                          loss_value.numpy()))\n",
        "\n",
        "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "print(\"Step: {}, Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "                                  loss(model, features, labels).numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68eFNnfPKveb",
        "outputId": "492a71f8-66a5-46c5-c1b1-edc0bdd6122f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, Initial Loss: 25657.927734375\n",
            "Step: 1, Loss: 25635.466796875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have defined another function to manage the backpropogation. We use gradiant tape to record the loss and the change in our variables. We use Adam (with a learning rate of 0.01) as our optimiser.\n",
        "\n",
        "Lastly we run a single gradient update to our mode. A modest improvement but we can do better.\n",
        "\n",
        "Next we will train our model. We'll run for 600 epochs (its a small dataset) so 12,000 iterations. To keep an eye on things and track progress we will print the loss to screen every 75 epochs:"
      ],
      "metadata": {
        "id": "yGGFEKE_LV30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 600\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "  # Training loop - using batches of 50\n",
        "  for x, y in train_batch:\n",
        "    # Optimize the model\n",
        "    loss_value, grads = grad(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # Track progress\n",
        "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
        "\n",
        "    # End epoch\n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "\n",
        "  if epoch % 75 == 0:\n",
        "    print(f\"Epoch {epoch}: Loss: {epoch_loss_avg.result()}\")\n",
        "\n",
        "# Print the final epoch loss\n",
        "print(f\"Epoch {epoch}: Loss: {epoch_loss_avg.result()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ9vvVpSLWJD",
        "outputId": "86b2459a-e6c0-4227-b25c-022cc16b8ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss: 29102.8515625\n",
            "Epoch 75: Loss: 2999.344970703125\n",
            "Epoch 150: Loss: 2911.3115234375\n",
            "Epoch 225: Loss: 2818.41455078125\n",
            "Epoch 300: Loss: 2765.358642578125\n",
            "Epoch 375: Loss: 2713.15185546875\n",
            "Epoch 450: Loss: 2665.478759765625\n",
            "Epoch 525: Loss: 2582.97607421875\n",
            "Epoch 599: Loss: 2513.35302734375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great - a working model. We can significant improvement early on - 29,160 MSE on epoch zero to 3,010 by epoch 75. After that, as we would expect, progress is slower but we keep improving. Of course we could run for many more epochs but this is just for fun. Let's see how it does on the test data: "
      ],
      "metadata": {
        "id": "6LXqxe9jOU9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = test_dataset.batch(10)\n",
        "\n",
        "for (x, y) in test_batch:\n",
        "  y_pred = model(x)\n",
        "  for pred, real in zip(y_pred, y):\n",
        "    print(f\"Predicted: {pred[0]};    Real: {real}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rHdKZKuOVI1",
        "outputId": "4101876f-948b-440f-cfba-37c79c96138e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 151.572998046875;    Real: 175.0\n",
            "Predicted: 77.31839752197266;    Real: 93.0\n",
            "Predicted: 173.27821350097656;    Real: 168.0\n",
            "Predicted: 252.98672485351562;    Real: 275.0\n",
            "Predicted: 169.44113159179688;    Real: 293.0\n",
            "Predicted: 302.7404479980469;    Real: 281.0\n",
            "Predicted: 108.14032745361328;    Real: 72.0\n",
            "Predicted: 169.71478271484375;    Real: 140.0\n",
            "Predicted: 201.80247497558594;    Real: 189.0\n",
            "Predicted: 170.5944366455078;    Real: 181.0\n",
            "Predicted: 147.5943145751953;    Real: 209.0\n",
            "Predicted: 106.42693328857422;    Real: 136.0\n",
            "Predicted: 255.49354553222656;    Real: 261.0\n",
            "Predicted: 113.81620025634766;    Real: 113.0\n",
            "Predicted: 177.01800537109375;    Real: 131.0\n",
            "Predicted: 163.92845153808594;    Real: 174.0\n",
            "Predicted: 220.10789489746094;    Real: 257.0\n",
            "Predicted: 144.87799072265625;    Real: 55.0\n",
            "Predicted: 109.49939727783203;    Real: 84.0\n",
            "Predicted: 88.90261840820312;    Real: 42.0\n",
            "Predicted: 114.27027130126953;    Real: 146.0\n",
            "Predicted: 211.2976531982422;    Real: 212.0\n",
            "Predicted: 193.49526977539062;    Real: 233.0\n",
            "Predicted: 149.95298767089844;    Real: 91.0\n",
            "Predicted: 166.53314208984375;    Real: 111.0\n",
            "Predicted: 127.03609466552734;    Real: 152.0\n",
            "Predicted: 176.4424285888672;    Real: 120.0\n",
            "Predicted: 104.28308868408203;    Real: 67.0\n",
            "Predicted: 311.74981689453125;    Real: 310.0\n",
            "Predicted: 117.13921356201172;    Real: 94.0\n",
            "Predicted: 126.3492660522461;    Real: 183.0\n",
            "Predicted: 151.69297790527344;    Real: 66.0\n",
            "Predicted: 234.23219299316406;    Real: 173.0\n",
            "Predicted: 76.39063262939453;    Real: 72.0\n",
            "Predicted: 96.97271728515625;    Real: 49.0\n",
            "Predicted: 123.0562973022461;    Real: 64.0\n",
            "Predicted: 78.15736389160156;    Real: 48.0\n",
            "Predicted: 198.32040405273438;    Real: 178.0\n",
            "Predicted: 95.38078308105469;    Real: 104.0\n",
            "Predicted: 117.74433135986328;    Real: 132.0\n",
            "Predicted: 217.64683532714844;    Real: 220.0\n",
            "Predicted: 90.40522766113281;    Real: 57.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostly these seem to be pretty good predicitons ... well done you. To end the tutorial let's run this again but using some of the simplified Keras tools:"
      ],
      "metadata": {
        "id": "Md43yWDgS2u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras implementation\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(features, labels, epochs=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ut41gsS3BW",
        "outputId": "ef0e6057-4bee-413e-dd83-2fa0766c963c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "2/2 [==============================] - 1s 13ms/step - loss: 2482.7896 - mse: 2482.7896\n",
            "Epoch 2/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2473.1726 - mse: 2473.1726\n",
            "Epoch 3/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2459.5044 - mse: 2459.5044\n",
            "Epoch 4/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2451.8579 - mse: 2451.8579\n",
            "Epoch 5/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2442.4080 - mse: 2442.4080\n",
            "Epoch 6/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2435.2974 - mse: 2435.2974\n",
            "Epoch 7/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2425.4478 - mse: 2425.4478\n",
            "Epoch 8/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2418.6113 - mse: 2418.6113\n",
            "Epoch 9/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2411.6338 - mse: 2411.6338\n",
            "Epoch 10/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2403.5435 - mse: 2403.5435\n",
            "Epoch 11/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2395.0752 - mse: 2395.0752\n",
            "Epoch 12/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2388.3340 - mse: 2388.3340\n",
            "Epoch 13/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2381.2102 - mse: 2381.2102\n",
            "Epoch 14/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2375.6157 - mse: 2375.6157\n",
            "Epoch 15/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2367.9678 - mse: 2367.9678\n",
            "Epoch 16/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2361.0867 - mse: 2361.0867\n",
            "Epoch 17/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2355.9380 - mse: 2355.9380\n",
            "Epoch 18/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2348.0757 - mse: 2348.0757\n",
            "Epoch 19/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2341.5667 - mse: 2341.5667\n",
            "Epoch 20/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2335.3003 - mse: 2335.3003\n",
            "Epoch 21/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2327.6382 - mse: 2327.6382\n",
            "Epoch 22/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2322.4822 - mse: 2322.4822\n",
            "Epoch 23/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2315.3813 - mse: 2315.3813\n",
            "Epoch 24/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2309.3293 - mse: 2309.3293\n",
            "Epoch 25/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2304.2676 - mse: 2304.2676\n",
            "Epoch 26/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2295.7498 - mse: 2295.7498\n",
            "Epoch 27/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2291.4424 - mse: 2291.4424\n",
            "Epoch 28/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2283.7578 - mse: 2283.7578\n",
            "Epoch 29/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2277.8948 - mse: 2277.8948\n",
            "Epoch 30/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2273.2117 - mse: 2273.2117\n",
            "Epoch 31/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2266.2825 - mse: 2266.2825\n",
            "Epoch 32/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2259.1589 - mse: 2259.1589\n",
            "Epoch 33/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2255.0271 - mse: 2255.0271\n",
            "Epoch 34/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2247.5210 - mse: 2247.5210\n",
            "Epoch 35/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2241.3496 - mse: 2241.3496\n",
            "Epoch 36/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2236.5042 - mse: 2236.5042\n",
            "Epoch 37/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2229.9343 - mse: 2229.9343\n",
            "Epoch 38/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2225.5747 - mse: 2225.5747\n",
            "Epoch 39/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2219.2856 - mse: 2219.2856\n",
            "Epoch 40/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2213.8948 - mse: 2213.8948\n",
            "Epoch 41/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2209.5459 - mse: 2209.5459\n",
            "Epoch 42/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2204.3579 - mse: 2204.3579\n",
            "Epoch 43/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2199.2510 - mse: 2199.2510\n",
            "Epoch 44/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2194.9968 - mse: 2194.9968\n",
            "Epoch 45/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2190.6777 - mse: 2190.6777\n",
            "Epoch 46/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2186.4995 - mse: 2186.4995\n",
            "Epoch 47/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2184.2588 - mse: 2184.2588\n",
            "Epoch 48/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2178.4517 - mse: 2178.4517\n",
            "Epoch 49/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2173.5476 - mse: 2173.5476\n",
            "Epoch 50/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2168.7097 - mse: 2168.7097\n",
            "Epoch 51/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2163.9177 - mse: 2163.9177\n",
            "Epoch 52/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2160.2920 - mse: 2160.2920\n",
            "Epoch 53/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2156.2239 - mse: 2156.2239\n",
            "Epoch 54/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2151.1460 - mse: 2151.1460\n",
            "Epoch 55/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2147.9934 - mse: 2147.9934\n",
            "Epoch 56/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2143.8188 - mse: 2143.8188\n",
            "Epoch 57/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2138.7676 - mse: 2138.7676\n",
            "Epoch 58/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2135.4941 - mse: 2135.4941\n",
            "Epoch 59/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2130.2549 - mse: 2130.2549\n",
            "Epoch 60/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2127.8662 - mse: 2127.8662\n",
            "Epoch 61/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2122.5122 - mse: 2122.5122\n",
            "Epoch 62/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2118.5188 - mse: 2118.5188\n",
            "Epoch 63/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2114.5779 - mse: 2114.5779\n",
            "Epoch 64/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2112.4553 - mse: 2112.4553\n",
            "Epoch 65/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2105.9565 - mse: 2105.9565\n",
            "Epoch 66/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2102.6917 - mse: 2102.6917\n",
            "Epoch 67/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2098.7493 - mse: 2098.7493\n",
            "Epoch 68/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2094.5391 - mse: 2094.5391\n",
            "Epoch 69/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2091.5220 - mse: 2091.5220\n",
            "Epoch 70/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2088.1282 - mse: 2088.1282\n",
            "Epoch 71/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2083.8955 - mse: 2083.8955\n",
            "Epoch 72/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2080.9353 - mse: 2080.9353\n",
            "Epoch 73/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2075.7024 - mse: 2075.7024\n",
            "Epoch 74/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2072.8184 - mse: 2072.8184\n",
            "Epoch 75/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2068.6555 - mse: 2068.6555\n",
            "Epoch 76/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2066.2415 - mse: 2066.2415\n",
            "Epoch 77/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2061.3804 - mse: 2061.3804\n",
            "Epoch 78/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2058.2776 - mse: 2058.2776\n",
            "Epoch 79/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2054.3696 - mse: 2054.3696\n",
            "Epoch 80/300\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2051.3887 - mse: 2051.3887\n",
            "Epoch 81/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2046.9795 - mse: 2046.9795\n",
            "Epoch 82/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2043.9772 - mse: 2043.9772\n",
            "Epoch 83/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2039.7828 - mse: 2039.7828\n",
            "Epoch 84/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2036.6846 - mse: 2036.6846\n",
            "Epoch 85/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2032.4825 - mse: 2032.4825\n",
            "Epoch 86/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2027.8796 - mse: 2027.8796\n",
            "Epoch 87/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2024.3209 - mse: 2024.3209\n",
            "Epoch 88/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2020.6295 - mse: 2020.6295\n",
            "Epoch 89/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2016.6218 - mse: 2016.6218\n",
            "Epoch 90/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2012.4877 - mse: 2012.4877\n",
            "Epoch 91/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2009.1250 - mse: 2009.1250\n",
            "Epoch 92/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2004.9727 - mse: 2004.9727\n",
            "Epoch 93/300\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2001.8313 - mse: 2001.8313\n",
            "Epoch 94/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1998.0647 - mse: 1998.0647\n",
            "Epoch 95/300\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1993.1836 - mse: 1993.1836\n",
            "Epoch 96/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1990.2103 - mse: 1990.2103\n",
            "Epoch 97/300\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1986.1069 - mse: 1986.1069\n",
            "Epoch 98/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1981.7150 - mse: 1981.7150\n",
            "Epoch 99/300\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1978.1547 - mse: 1978.1547\n",
            "Epoch 100/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1974.5737 - mse: 1974.5737\n",
            "Epoch 101/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1971.2032 - mse: 1971.2032\n",
            "Epoch 102/300\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1966.8458 - mse: 1966.8458\n",
            "Epoch 103/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1962.9250 - mse: 1962.9250\n",
            "Epoch 104/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1959.5969 - mse: 1959.5969\n",
            "Epoch 105/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1956.6847 - mse: 1956.6847\n",
            "Epoch 106/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1952.3293 - mse: 1952.3293\n",
            "Epoch 107/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1948.6022 - mse: 1948.6022\n",
            "Epoch 108/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1944.6941 - mse: 1944.6941\n",
            "Epoch 109/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1941.6038 - mse: 1941.6038\n",
            "Epoch 110/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1937.7638 - mse: 1937.7638\n",
            "Epoch 111/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1933.7290 - mse: 1933.7290\n",
            "Epoch 112/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1930.4028 - mse: 1930.4028\n",
            "Epoch 113/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1926.7200 - mse: 1926.7200\n",
            "Epoch 114/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1923.1538 - mse: 1923.1536\n",
            "Epoch 115/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1919.5378 - mse: 1919.5378\n",
            "Epoch 116/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1916.8728 - mse: 1916.8728\n",
            "Epoch 117/300\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1913.0642 - mse: 1913.0642\n",
            "Epoch 118/300\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 1911.0021 - mse: 1911.0021\n",
            "Epoch 119/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1906.6691 - mse: 1906.6691\n",
            "Epoch 120/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1903.3218 - mse: 1903.3218\n",
            "Epoch 121/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1900.2047 - mse: 1900.2047\n",
            "Epoch 122/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1896.8506 - mse: 1896.8506\n",
            "Epoch 123/300\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1894.2791 - mse: 1894.2791\n",
            "Epoch 124/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1890.9535 - mse: 1890.9535\n",
            "Epoch 125/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1889.1976 - mse: 1889.1976\n",
            "Epoch 126/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1884.6122 - mse: 1884.6122\n",
            "Epoch 127/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1882.1025 - mse: 1882.1025\n",
            "Epoch 128/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1879.2583 - mse: 1879.2583\n",
            "Epoch 129/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1876.4465 - mse: 1876.4465\n",
            "Epoch 130/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1873.6488 - mse: 1873.6488\n",
            "Epoch 131/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1870.6313 - mse: 1870.6313\n",
            "Epoch 132/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1867.8796 - mse: 1867.8796\n",
            "Epoch 133/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1865.6244 - mse: 1865.6243\n",
            "Epoch 134/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1863.0981 - mse: 1863.0981\n",
            "Epoch 135/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1860.1246 - mse: 1860.1246\n",
            "Epoch 136/300\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1858.8260 - mse: 1858.8260\n",
            "Epoch 137/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1855.7133 - mse: 1855.7133\n",
            "Epoch 138/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1853.0621 - mse: 1853.0621\n",
            "Epoch 139/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1851.1575 - mse: 1851.1575\n",
            "Epoch 140/300\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1847.5016 - mse: 1847.5016\n",
            "Epoch 141/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1845.4330 - mse: 1845.4330\n",
            "Epoch 142/300\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1843.2825 - mse: 1843.2825\n",
            "Epoch 143/300\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1840.3148 - mse: 1840.3148\n",
            "Epoch 144/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1838.8309 - mse: 1838.8309\n",
            "Epoch 145/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1836.3796 - mse: 1836.3796\n",
            "Epoch 146/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1833.4464 - mse: 1833.4464\n",
            "Epoch 147/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1830.9039 - mse: 1830.9039\n",
            "Epoch 148/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1828.9348 - mse: 1828.9348\n",
            "Epoch 149/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1826.9042 - mse: 1826.9042\n",
            "Epoch 150/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1824.3900 - mse: 1824.3900\n",
            "Epoch 151/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1821.3511 - mse: 1821.3511\n",
            "Epoch 152/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1818.9442 - mse: 1818.9442\n",
            "Epoch 153/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1816.3544 - mse: 1816.3544\n",
            "Epoch 154/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1813.5096 - mse: 1813.5096\n",
            "Epoch 155/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1811.0643 - mse: 1811.0643\n",
            "Epoch 156/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1808.4797 - mse: 1808.4797\n",
            "Epoch 157/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1806.4597 - mse: 1806.4597\n",
            "Epoch 158/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1803.2834 - mse: 1803.2834\n",
            "Epoch 159/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1801.6078 - mse: 1801.6078\n",
            "Epoch 160/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1798.8207 - mse: 1798.8207\n",
            "Epoch 161/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1797.2791 - mse: 1797.2791\n",
            "Epoch 162/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1794.7340 - mse: 1794.7340\n",
            "Epoch 163/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1792.4625 - mse: 1792.4625\n",
            "Epoch 164/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1790.7550 - mse: 1790.7550\n",
            "Epoch 165/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1787.6232 - mse: 1787.6232\n",
            "Epoch 166/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1785.8170 - mse: 1785.8170\n",
            "Epoch 167/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1783.2214 - mse: 1783.2214\n",
            "Epoch 168/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1781.4241 - mse: 1781.4241\n",
            "Epoch 169/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1778.4922 - mse: 1778.4922\n",
            "Epoch 170/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1776.6012 - mse: 1776.6012\n",
            "Epoch 171/300\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1774.7410 - mse: 1774.7410\n",
            "Epoch 172/300\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1772.4048 - mse: 1772.4048\n",
            "Epoch 173/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1770.5103 - mse: 1770.5103\n",
            "Epoch 174/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1767.9202 - mse: 1767.9202\n",
            "Epoch 175/300\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1765.7979 - mse: 1765.7979\n",
            "Epoch 176/300\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1764.1372 - mse: 1764.1372\n",
            "Epoch 177/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1761.7106 - mse: 1761.7106\n",
            "Epoch 178/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1759.5647 - mse: 1759.5647\n",
            "Epoch 179/300\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1757.2797 - mse: 1757.2797\n",
            "Epoch 180/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1755.0541 - mse: 1755.0541\n",
            "Epoch 181/300\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1752.9429 - mse: 1752.9429\n",
            "Epoch 182/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1751.1942 - mse: 1751.1942\n",
            "Epoch 183/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1749.0234 - mse: 1749.0234\n",
            "Epoch 184/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1747.4116 - mse: 1747.4116\n",
            "Epoch 185/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1745.9918 - mse: 1745.9918\n",
            "Epoch 186/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1743.7800 - mse: 1743.7800\n",
            "Epoch 187/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1741.7494 - mse: 1741.7494\n",
            "Epoch 188/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1739.8784 - mse: 1739.8784\n",
            "Epoch 189/300\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1738.0060 - mse: 1738.0060\n",
            "Epoch 190/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1736.1829 - mse: 1736.1829\n",
            "Epoch 191/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1734.1652 - mse: 1734.1652\n",
            "Epoch 192/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1732.1190 - mse: 1732.1190\n",
            "Epoch 193/300\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1729.7593 - mse: 1729.7593\n",
            "Epoch 194/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1727.7180 - mse: 1727.7180\n",
            "Epoch 195/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1725.8732 - mse: 1725.8732\n",
            "Epoch 196/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1724.7834 - mse: 1724.7834\n",
            "Epoch 197/300\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1721.9385 - mse: 1721.9385\n",
            "Epoch 198/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1719.7920 - mse: 1719.7920\n",
            "Epoch 199/300\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1717.9547 - mse: 1717.9547\n",
            "Epoch 200/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1716.3638 - mse: 1716.3638\n",
            "Epoch 201/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1714.4585 - mse: 1714.4585\n",
            "Epoch 202/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1712.9689 - mse: 1712.9689\n",
            "Epoch 203/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1711.4890 - mse: 1711.4890\n",
            "Epoch 204/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1709.8547 - mse: 1709.8547\n",
            "Epoch 205/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1707.4988 - mse: 1707.4988\n",
            "Epoch 206/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1705.4812 - mse: 1705.4812\n",
            "Epoch 207/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1704.4554 - mse: 1704.4554\n",
            "Epoch 208/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1702.7590 - mse: 1702.7590\n",
            "Epoch 209/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1701.2750 - mse: 1701.2750\n",
            "Epoch 210/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1700.0707 - mse: 1700.0707\n",
            "Epoch 211/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1698.9067 - mse: 1698.9067\n",
            "Epoch 212/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1696.9895 - mse: 1696.9895\n",
            "Epoch 213/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1694.7632 - mse: 1694.7632\n",
            "Epoch 214/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1694.2518 - mse: 1694.2518\n",
            "Epoch 215/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1691.8768 - mse: 1691.8768\n",
            "Epoch 216/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1690.7504 - mse: 1690.7504\n",
            "Epoch 217/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1688.4486 - mse: 1688.4486\n",
            "Epoch 218/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1686.6606 - mse: 1686.6606\n",
            "Epoch 219/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1684.8149 - mse: 1684.8149\n",
            "Epoch 220/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1683.6793 - mse: 1683.6793\n",
            "Epoch 221/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1682.9587 - mse: 1682.9587\n",
            "Epoch 222/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1680.5541 - mse: 1680.5541\n",
            "Epoch 223/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1679.2009 - mse: 1679.2009\n",
            "Epoch 224/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1677.7635 - mse: 1677.7635\n",
            "Epoch 225/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1676.1049 - mse: 1676.1049\n",
            "Epoch 226/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1674.0828 - mse: 1674.0828\n",
            "Epoch 227/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1672.4209 - mse: 1672.4209\n",
            "Epoch 228/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1670.7925 - mse: 1670.7925\n",
            "Epoch 229/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1669.4091 - mse: 1669.4091\n",
            "Epoch 230/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1668.1985 - mse: 1668.1985\n",
            "Epoch 231/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1666.5010 - mse: 1666.5010\n",
            "Epoch 232/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1665.1406 - mse: 1665.1406\n",
            "Epoch 233/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1665.7462 - mse: 1665.7462\n",
            "Epoch 234/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1663.3304 - mse: 1663.3304\n",
            "Epoch 235/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1660.9928 - mse: 1660.9928\n",
            "Epoch 236/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1659.6006 - mse: 1659.6006\n",
            "Epoch 237/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1658.1099 - mse: 1658.1099\n",
            "Epoch 238/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1656.5875 - mse: 1656.5875\n",
            "Epoch 239/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1655.2115 - mse: 1655.2115\n",
            "Epoch 240/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1653.6865 - mse: 1653.6865\n",
            "Epoch 241/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1652.4806 - mse: 1652.4806\n",
            "Epoch 242/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1650.9297 - mse: 1650.9297\n",
            "Epoch 243/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1650.0491 - mse: 1650.0491\n",
            "Epoch 244/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1648.1713 - mse: 1648.1713\n",
            "Epoch 245/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1647.6383 - mse: 1647.6383\n",
            "Epoch 246/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1645.4194 - mse: 1645.4194\n",
            "Epoch 247/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1644.4039 - mse: 1644.4039\n",
            "Epoch 248/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1644.1902 - mse: 1644.1902\n",
            "Epoch 249/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1642.5277 - mse: 1642.5277\n",
            "Epoch 250/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1640.6156 - mse: 1640.6156\n",
            "Epoch 251/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1639.4281 - mse: 1639.4281\n",
            "Epoch 252/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1638.4078 - mse: 1638.4078\n",
            "Epoch 253/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1637.9581 - mse: 1637.9581\n",
            "Epoch 254/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1635.4926 - mse: 1635.4926\n",
            "Epoch 255/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1634.0969 - mse: 1634.0969\n",
            "Epoch 256/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1632.8892 - mse: 1632.8892\n",
            "Epoch 257/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1631.4399 - mse: 1631.4399\n",
            "Epoch 258/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1630.1959 - mse: 1630.1959\n",
            "Epoch 259/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1629.6384 - mse: 1629.6384\n",
            "Epoch 260/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1627.9017 - mse: 1627.9017\n",
            "Epoch 261/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1626.4056 - mse: 1626.4056\n",
            "Epoch 262/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1625.8007 - mse: 1625.8007\n",
            "Epoch 263/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1624.4215 - mse: 1624.4215\n",
            "Epoch 264/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1622.8250 - mse: 1622.8250\n",
            "Epoch 265/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1621.4058 - mse: 1621.4058\n",
            "Epoch 266/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1620.3812 - mse: 1620.3812\n",
            "Epoch 267/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1619.2677 - mse: 1619.2677\n",
            "Epoch 268/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1617.5465 - mse: 1617.5465\n",
            "Epoch 269/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1616.2716 - mse: 1616.2716\n",
            "Epoch 270/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1616.9636 - mse: 1616.9636\n",
            "Epoch 271/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1613.4764 - mse: 1613.4764\n",
            "Epoch 272/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1612.4515 - mse: 1612.4515\n",
            "Epoch 273/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1611.0543 - mse: 1611.0543\n",
            "Epoch 274/300\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1609.5858 - mse: 1609.5858\n",
            "Epoch 275/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1609.7770 - mse: 1609.7770\n",
            "Epoch 276/300\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1607.5044 - mse: 1607.5044\n",
            "Epoch 277/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1606.8158 - mse: 1606.8158\n",
            "Epoch 278/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1605.0686 - mse: 1605.0686\n",
            "Epoch 279/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1604.3143 - mse: 1604.3143\n",
            "Epoch 280/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1603.2673 - mse: 1603.2673\n",
            "Epoch 281/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1601.5563 - mse: 1601.5563\n",
            "Epoch 282/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1600.1400 - mse: 1600.1400\n",
            "Epoch 283/300\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1598.8071 - mse: 1598.8071\n",
            "Epoch 284/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1597.5344 - mse: 1597.5344\n",
            "Epoch 285/300\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1596.1785 - mse: 1596.1785\n",
            "Epoch 286/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1595.2476 - mse: 1595.2476\n",
            "Epoch 287/300\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1593.9515 - mse: 1593.9515\n",
            "Epoch 288/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1593.4810 - mse: 1593.4810\n",
            "Epoch 289/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1592.1978 - mse: 1592.1978\n",
            "Epoch 290/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1590.8119 - mse: 1590.8119\n",
            "Epoch 291/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1590.2384 - mse: 1590.2384\n",
            "Epoch 292/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1588.5413 - mse: 1588.5413\n",
            "Epoch 293/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1588.6637 - mse: 1588.6637\n",
            "Epoch 294/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1587.1707 - mse: 1587.1707\n",
            "Epoch 295/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1585.7362 - mse: 1585.7362\n",
            "Epoch 296/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1584.8315 - mse: 1584.8315\n",
            "Epoch 297/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1583.4803 - mse: 1583.4803\n",
            "Epoch 298/300\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1582.4379 - mse: 1582.4379\n",
            "Epoch 299/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1581.8740 - mse: 1581.8740\n",
            "Epoch 300/300\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1580.0519 - mse: 1580.0519\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5f3b8a410>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing ..."
      ],
      "metadata": {
        "id": "ptLFZeg7bWH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = test_dataset.batch(42) # the whole dataset\n",
        "test_features, test_labels = next(iter(test_batch))\n",
        "\n",
        "test_loss, test_mse = model.evaluate(test_features,  test_labels, verbose=2)\n",
        "print('\\nTest MSE:', test_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1loaaISxT3kQ",
        "outputId": "521e10d3-1e29-413b-f1bb-dacd2b49ce04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 - 0s - loss: 1653.4844 - mse: 1653.4844 - 159ms/epoch - 80ms/step\n",
            "\n",
            "Test MSE: 1653.484375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions ..."
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_features)\n",
        "for pred, real in zip(y_pred, test_labels):\n",
        "    print(f\"Predicted: {pred[0]};    Real: {real}\")\n",
        "\n",
        "_, rmse = model.evaluate(test_features, test_labels, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "07ca5a0b-1194-4a55-d6d2-d63977a81ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 165.4960479736328;    Real: 175.0\n",
            "Predicted: 81.77456665039062;    Real: 93.0\n",
            "Predicted: 144.0576171875;    Real: 168.0\n",
            "Predicted: 267.83673095703125;    Real: 275.0\n",
            "Predicted: 182.22000122070312;    Real: 293.0\n",
            "Predicted: 313.6932067871094;    Real: 281.0\n",
            "Predicted: 108.27365112304688;    Real: 72.0\n",
            "Predicted: 149.18771362304688;    Real: 140.0\n",
            "Predicted: 200.7175750732422;    Real: 189.0\n",
            "Predicted: 169.13662719726562;    Real: 181.0\n",
            "Predicted: 154.9176025390625;    Real: 209.0\n",
            "Predicted: 110.21495056152344;    Real: 136.0\n",
            "Predicted: 239.6842041015625;    Real: 261.0\n",
            "Predicted: 112.23800659179688;    Real: 113.0\n",
            "Predicted: 163.11038208007812;    Real: 131.0\n",
            "Predicted: 178.48025512695312;    Real: 174.0\n",
            "Predicted: 209.123291015625;    Real: 257.0\n",
            "Predicted: 148.43150329589844;    Real: 55.0\n",
            "Predicted: 109.24411010742188;    Real: 84.0\n",
            "Predicted: 77.59857940673828;    Real: 42.0\n",
            "Predicted: 75.47003936767578;    Real: 146.0\n",
            "Predicted: 192.97250366210938;    Real: 212.0\n",
            "Predicted: 226.9222869873047;    Real: 233.0\n",
            "Predicted: 127.52832794189453;    Real: 91.0\n",
            "Predicted: 140.13011169433594;    Real: 111.0\n",
            "Predicted: 127.72693634033203;    Real: 152.0\n",
            "Predicted: 131.85069274902344;    Real: 120.0\n",
            "Predicted: 77.82752990722656;    Real: 67.0\n",
            "Predicted: 281.3627624511719;    Real: 310.0\n",
            "Predicted: 126.15125274658203;    Real: 94.0\n",
            "Predicted: 124.23958587646484;    Real: 183.0\n",
            "Predicted: 134.4253387451172;    Real: 66.0\n",
            "Predicted: 221.58514404296875;    Real: 173.0\n",
            "Predicted: 82.62057495117188;    Real: 72.0\n",
            "Predicted: 96.42882537841797;    Real: 49.0\n",
            "Predicted: 119.85530853271484;    Real: 64.0\n",
            "Predicted: 77.55780029296875;    Real: 48.0\n",
            "Predicted: 178.19833374023438;    Real: 178.0\n",
            "Predicted: 48.517295837402344;    Real: 104.0\n",
            "Predicted: 87.22867584228516;    Real: 132.0\n",
            "Predicted: 255.73333740234375;    Real: 220.0\n",
            "Predicted: 102.78599548339844;    Real: 57.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall very comparable results. One neural network down ... well done 👊"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    }
  ]
}